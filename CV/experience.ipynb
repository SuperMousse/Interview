1. https://zhuanlan.zhihu.com/p/75007429

BN的基本原理，其中的两个参数的作用？

BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度。使得模型对网络的参数不那么敏感，使学习更加稳定。可以缓解梯度消失的问题。

BN的两个参数为经过不断学习学习出来的还原特征的平移和缩放的因子，用于对特征的分布的还原，这些操作均在BN层中完成。

过拟合的解决办法？

Early stopping

数据集扩增（增强等）

正则化方法

Dropout

梯度爆炸的原因和解决方案？

1.选择了不恰当的损失函数

2.学习率过高

3.模型问题（网络模型过深）

解决方法：

1.控制反向传播的范围，clap住。

2.使用batchnorm、预训练加微调（模型从0训练比较困难）

3.调小学习率

4.添加权重（W）正则化项对loss做约束，梯度过大的时候进行惩罚。

5.残差结构

梯度消失的原因和解决方案？

1.选择了不恰当的损失函数

2.选择了不恰当的激活函数，如sigmoid

3.网络的层数过深

解决方法：

1.使用batchnorm、预训练加微调（模型从0训练比较困难）

2.选择relu、leaky relu等激活函数

3.残差结构

激活函数如果不是以0为中心的影响: 以sigmoid为例, 其导数为sigmoid(x)(1-sigmoid(x)), 导数恒为正值, 那么其梯度方向要么


优点：勤奋努力（高考）、适应环境快（研究方向多元）、待人比较温和（容易交朋友）、抗压能力强（身兼多职）

缺点：不太清楚如何拒绝别人（因此耽误了自己项目进度）、过于追求明白原理（有时项目进度会慢）
