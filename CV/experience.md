1. https://zhuanlan.zhihu.com/p/75007429  

(1). BN的基本原理，其中的两个参数的作用?  
   a. BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度。使得模型对网络的参数不那么敏感，使学习更加稳定。可以缓解梯度消失的问题。  
   b. BN的两个参数为经过不断学习学习出来的还原特征的平移和缩放的因子，用于对特征的分布的还原，这些操作均在BN层中完成。  

(2). 过拟合的解决办法?    
   a. Early stopping  
   b. 数据集扩增（增强等）  
   c. 正则化方法   
   d. Dropout  

(3). 梯度爆炸的原因和解决方案？ 
原因: a. 选择了不恰当的损失函数   
     b. 学习率过高   
     c. 模型问题（网络模型过深）  
解决方法：a.控制反向传播的范围，梯度Clip   
        b.使用batchnorm、预训练加微调（模型从0训练比较困难）   
        c.调小学习率   
        d.添加权重W正则化项对loss做约束，梯度过大的时候进行惩罚。   
        e.残差结构   

(4) 梯度消失的原因和解决方案？ 
原因: a.选择了不恰当的损失函数   
     b.选择了不恰当的激活函数，如sigmoid   
     c.网络的层数过深  
解决方法： 
      a.使用batchnorm、预训练加微调（模型从0训练比较困难） 
      b.选择relu、leaky relu等激活函数  
      c.残差结构

(5) 激活函数如果不是以0为中心的影响:  降低模型收敛速度  https://liam.page/2018/04/17/zero-centered-active-function/
以sigmoid为例, 其输出值恒为正数
f(z) = \sum{w_i * x_i + b} 
=> 对w_i偏导: x_i * (Loss对f偏导) * (f对z偏导)
因此当前层的不同w_i, 其梯度由前一层的输出值影响, 若前一层为sigmoid，则不同x_i均为正值, 则当前层的参数更新只能全正/全负, 如果有的参数需要正
向更新, 有的参数需要负向更新, 那么就需要z字型绕很久, 才能收敛到最优解 

优点：勤奋努力（高考）、适应环境快（研究方向多元）、待人比较温和（容易交朋友）、抗压能力强（身兼多职）
缺点：不太清楚如何拒绝别人（因此耽误了自己项目进度）、过于追求明白原理（有时项目进度会慢）



2. 目标检测相关  
(1) R-CNN  
(2) Fast R-CNN  
(3) Faster R-CNN
