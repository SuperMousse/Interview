1. k nearest neighbours

2. principal component analysis: 无监督学习, 放这为了和线性判别分析做对比
   用处: 高维数据可视化(投影到二维或者三维);  
        作为数据预处理方法用在图像检索等问题中;  
        人脸识别特征脸(人脸数据集分解为不同的特征向量, 排列后就是特征脸, 每幅图可以看成特征脸的线性组合)

   输入：n维样本集X={x_1, x_2, ..., x_n}，要降维到的维数$n^{'}$.  
   输出：降维后的样本集Y  
   
    a.对所有的样本进行中心化 x_i = x_i - 1/m \sum{x_j}  
    b.计算样本的协方差矩阵C = 1/n XX^T  
    c.求出协方差矩阵的特征值及对应的特征向量  
    d.将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P  
    e.Y=PX即为降维到k维后的数据  

3. linear discriminant analysis: 不同于PCA, 是监督学习方法, 又叫Fisher线性判别    
   核心: 将所有的数据投影到一个高维直线上, 使得投影后类内方差最小，类间方差最大  
   计算方法:  
   a. 计算类内均值u1, u2    
   b. 计算类内方差s1, s2, 类内总离散矩阵s=s1+s2, 求s^{-1}   
   c. w = s^{-1}(u1-u2)  
   d. y = w^{T}x, 判断是否大于(w^{T}u1 + w^{T}u2) / 2  

   推导: u1 = \sum{x_i}, u2 = \sum{x_i}, S1^{2} = \sum{(x_i-u1)^{T}(x_i-u1)}, S2^{2} = \sum{(x_i-u2)^{T}(x_i-u2)}  
   J(w) = |w^{T}(u1-u2)|^{2} / (S1^2+S2^{2}) = w^{T}S_{b}w / w^{T}S_{w}w   
   另分母为常数, 拉格朗日乘子法  J(w) = S_{b}w - lambda S_{w}w = 0, lambada S_{w} w = S_{b} w, lambda w = S_{w}^{-1} S_{b} w,  
   lambda w = S_{w}^{-1} (u1 - u2) (u1 - u2)^{T} w, lambda w = S_{w}^{-1} (u1 - u2) * constant, w = constant S_{w}^{-1} (u1 - u2)  
   S_{b}展开后w与u1-u0方向相同得出闭式解(u1-u2)/S{w}   
   
   LDA: 有监督的降维方法(需要根据类别来训练w),降维最多降到k-1维(w特征向量, 受到求解矩阵的秩的限制), 可以用于降维，还可以用于分类, 可以使用类别的先验知识, 更明确, 更能反映样本差异性  
   
   PCA: 无监督的降维方法, 降维多少没有限制, 只能用于降维, 选择样本点投影具有最大方差的方向, 目的较为模糊  
